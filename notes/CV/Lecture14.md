# Neural Networks 神经网络

前情提要：线性分类法是有极限的！（异或、菌落、圆环……）

想法：提取高维特征再使用线性分类器。即对原始数据进行特定的“**特征变换**”，可以让数据在新的特征空间中更容易被线性分类器分开。
例：分布在 $\{x^2+y^2\le1\}$ 的特征 $\Rightarrow$ 分布在 $\{\theta\in[0,2\pi),r\in[0,1]\}$ 的特征（极坐标变换，原始空间 $\rightarrow$ 特征空间）

CV 中的 **HoG 特征**可以提取图像各个局部的方向性边缘，对细微图像变化（缩放、旋转、光线变化等）具有鲁棒性，适合线性分类器（例如 SVM）

2011年 **ImageNet 挑战赛**（图像分类竞赛，2012年引入了 AlexNet 神经网络，深度学习模型在比赛中的正确率提升到一个前所未有的高度。）中，强调通过精确提取和压缩图像特征来应对高维问题，主要流程包括：

1. **多特征连接（Concatenate Features）：** 提取多种图像特征并拼接，使特征维度更高（但更能表征图像）。

2. **特征压缩（Compress）：**
   
   - 使用 PCA（主成分分析）降低维度，减少冗余信息。
   - 提取 Fisher Vector（描述概率分布的特征方法），进一步捕捉高维特征的分布信息。

3. **线性分类器：**
   
   - 使用 One-vs-All SVM（针对多分类问题，每个类别与其他类别分别训练分类器）。
   - 优化时使用随机梯度下降（Stochastic Gradient Descent, SGD）来学习分类器的权重。

在深度学习和神经网络普及之前，利用图像特征工程（如 Fisher Vector 和 PCA 特征压缩）以及线性分类器（SVM）来完成复杂的图像分类任务。

（但是，时代变了！）

## 神经网络

（生物学部分神经元的结构知识就不做笔记了）

### 1. Single-Layer Percetron（SLP，单层感知机）

单层感知机 SLP 是一种基于神经元的线性分类器：

- 输入：多个特征 $x_1,x_2,...,x_n$；

- 内部：每个特征对应的权重 $w_1,w_2,...,w_n$，以及一个偏置 $b$；

- 操作：激活函数 $f(\cdot)$；

- 输出：通常是二分类（0 or 1）。

$$
out = f(\sum_{i=1}^nw_i\cdot x_i+b)
$$

显然 SLPs 本质上就是一个线性分类器，无法解决非线性可分的问题，例如异或问题（XOR gate）。

> Minsky 和 Papert 在《Perceptrons》（1969）一书中证明了 SLP 无法学习 XOR，这项研究明确了单层感知机的限制。

结局：AI winter。

### 2. MultiLayer Perceptrons（MLPs，多层感知机）

**神经网络**是仿照生物神经元构建的多层神经网络结构，有一个**输入层（input layer）**、一个或多个**隐含层（hidden layer）**、一个**输出层（output layer）** 组成。

**多层感知机**在单层感知机上加入至少一个隐藏层，通过组合特征处理复杂的高维数据，并解决非线性问题。（属于神经网络的一种）

对于单层感知机，假设输入是 $x\in \mathbf{R}^D$，输出 $y\in\mathbf{R}^C$，那么其权重矩阵就是一个 $C\times D$ 的矩阵：$y=Wx$。

而多层感知机（假设有一个隐含层），有：

$$
y=W_2\sigma(W_1x)
$$

其中：

- $W_1\in\mathbf{R}^{H\times D}$ 是输入层与隐藏层之间的权重矩阵。

- $W_2\in\mathbf{R}^{C\times H}$ 是隐藏层与输出层之间的权重矩阵。

- $H$ 表示隐藏层的神经元数量。

- $\sigma$ 是一个 **非线性激活函数**（如 Sigmoid 或 ReLU），用于引入非线性。

激活函数是必不可少的，而且应该是可微的。

具有至少一个隐藏层的多层感知器（MLP）可以以任意精度逼近任何函数。

常见激活函数：

| 名称      | 函数                             | 名称         | 函数                                                                     |
| ------- | ------------------------------ | ---------- | ---------------------------------------------------------------------- |
| Sigmoid | $\sigma(x)=\frac{1}{1+e^{-x}}$ | Leaky ReLU | $\max(0.1x,x)$                                                         |
| tanh    | $\tanh(x)$                     | GELU       | $\approx x\sigma(1.702x)$                                              |
| ReLU    | $\max(0,x)$                    | ELU        | $f(x)=\begin{cases}x & x \geq 0 \\\alpha (e^x - 1) & x < 0\end{cases}$ |

MLP的局限性：MLP的通用逼近 → 大量隐藏神经元（宽度） → 深度神经网络
