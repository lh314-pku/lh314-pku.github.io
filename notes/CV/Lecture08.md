<div>

</div>

# Two-View Stereo双目立体

> 相机标定可以求出相机参数，对极几何便于找到对应点，三角测量根据相机参数和对应点还原出世界坐标，双目视觉与三角测量类似，只不过是要根据双目图计算出点的深度，有了这一套流程，就可以进行三维重建

双目立体：

- 输入：一组立体图像对，即通过两个相机（已标定）从不同视角拍摄的图像。

- 输出：Dense Depth Map，稠密深度图， 即对输入图像中每个像素都估计其到相机的深度距离。

## Basic Stereo Matching Algorithm 基本立体匹配算法

使用一对校准过的摄像头，我们可以通过以下方式重建三维信息：

- **对应关系** (Correspondence) + **三角测量** (Triangulation)

对第一张图像中的每一个像素：

- 在右图中找到对应的**极线** (Epipolar line)

- 检查该极线上的所有像素，找到最佳匹配

- 对匹配的点进行三角测量以获取深度信息

### 简单立体系统：

- 对极线 = 对应扫描线

- 相机的图像平面彼此平行且与基线平行

- 相机中心在同一高度，焦距相同（也就是相机左右平移得到）

- 对极线沿图像的水平扫描线分布

考虑其[本质矩阵](https://lh314-pku.github.io/notes/CV/Lecture07#2-本质矩阵)：$x'^TEx=0$，有 $E=[t_{\times}]R$。

由于相机之间没有旋转关系，只有平移关系，则：$R=I$，$t=(t,0,0)$（相机平移）

则有：

$$
E=[t_{\times}]R=\begin{bmatrix}
0&0&0\\0&0&-t\\0&t&0
\end{bmatrix}
$$

通过基本矩阵我们发现：对应点总是在同一高度下。

简单立体的**匹配算法**：

对第一幅图的每个像素，在同一高度下测试扫描线上的所有像素并查找最佳匹配，之后通过三角测量获得深度信息。

## 一般立体系统

对于一般的情况，图像平面不会平行，但是我们可以使用基础矩阵来找到[单应矩阵](https://lh314-pku.github.io/notes/CV/Lecture05#2-单应矩阵homography)，将每个视图投影到与基线平行的公共平面上（立体校正）

### Stereo Image Rectification立体视觉矫正

- 通过本质矩阵 or 基本矩阵计算旋转矩阵 $R_1$；

- 旋转右相机，使两个相机方向一致；

- 使用旋转矩阵 $R_{rect}$ 将两个相机/成像平面与基线平行，从而使得图像中的极线变平行。（也就是说让原始的极线（epipolar lines）映射到无穷远）

- 通过单应矩阵 $H$ 进一步调整，以减少相机畸变等失真。

> **单应矩阵**用来匹配两个并不完全重合的图片，将其投射在同一平面上。
> 
> 对极几何[相关概念](https://lh314-pku.github.io/notes/CV/Lecture07#epipolar-geometry-对极几何-1)

现在的问题在于：如何求解 $R_{rect}$?

矫正旋转矩阵 $R_{rect}$ 的目标是将相机的图像平面调整到与基线平行（视觉上来说，就是让从不同视角看到的同一物体变成类似于相机左右平移拍摄的效果）：

| Before    | ![](https://lh314-pku.github.io/notes/CV/images/Stereo_Before.png) |
| --------- | ------------------------------------------------------------------ |
| **After** | ![](https://lh314-pku.github.io/notes/CV/images/Stereo_After.png)  |

为了满足“极点无穷远”的条件，结合[基本矩阵](https://lh314-pku.github.io/notes/CV/Lecture07#3-基本矩阵)的条件，令 $Fe_1=0$ ，通过 SVD 分解计算极点 $e_1$ ，然后构造 $R_{rect}$：$R_{rect}=(\mathbf{r_1},\mathbf{r_2},\mathbf{r_3})^T$，其中：

$$
\mathbf{r_1}=\frac{e_1}{\|e_1\|},
\mathbf{r_2}=\frac{(0,0,1)^T\times\mathbf{r_1}}
{\|(0,0,1)^T\times\mathbf{r_1}\|},
\mathbf{r_3}=\mathbf{r_1}\times\mathbf{r_2}
$$

此时，$R_{rect}e_1=(\|e_1\|,0,0)^T$，这就代表着一个沿基线方向（前两个元素表示与x轴，即基线方向平行）的**无穷远点**（第三个元素为0）

### Depth from Disparity深度视差

得到了平行拍摄的图片，怎么计算图像的深度？我们可以考虑相似三角形，分为同侧和异侧两种情况：

![](https://lh314-pku.github.io/notes/CV/images/depth.png)

![](https://lh314-pku.github.io/notes/CV/images/depth1.png)

结果推导，图像的深度公式为：

$$
z=\frac{fB}{x-x'}
$$

其中：

- $f$：焦距

- $B$：相机间（即基线）距离。更大的基线代表了更好的**三角测量误差**和更困难的**匹配**，适合需要高精度深度估计的场景；小基线适合匹配内容相似的物体（可以抛弃一部分精度）

- $x,x'$：图像平面内的水平坐标（单位m），其差值称为**视差disparity**。

$$
disparity=\frac{Bf}{z}
$$

- 视差是针对于像素坐标而言的，如果最后想要求以 m 为单位的深度，要么把视差转换单位，要么使用以像素为单位的焦距；

- 深度与视差成倒数关系，一般我们会先求出视差图，再转换为深度图，视差图可以看作一个中间载体。

## Stereo Matching立体匹配

### Local Stereo Matching局部匹配

考虑一个 $K\times K$ 大小的匹配窗口 $w_L$，$w_R$，我们如何判断两个窗口的相似性？

- **Sum of Squared Differences（SSD，平方差和）**：计算窗口内所有匹配点的L2误差并求和。如果两者非常相似，差值会很小。

$$
SSD(x,y,d)=\|w_L(x,y)-w_R(x-d,y)\|_2^2
$$

SSD的优势在于其非常直观、符合直觉，但是其对于光照等因素过于敏感，不是更加鲁棒的判断。

- **Zero Normalized Cross-Correlation（ZNCC，零均值归一化相互关）**：相较于SSD，ZNCC先对窗口向量进行去均值操作，消除亮度的整体偏移；再计算两个去均值后的向量之间的相似性（点积），归一化到$[-1,1]$内。数值越高相似度越高。

$$
\text{ZNCC}(x, y, d) = 
\frac{ 
\big(\mathbf{w}_L(x, y) - \overline{\mathbf{w}}_L(x, y)\big)^T 
\big(\mathbf{w}_R(x - d, y) - \overline{\mathbf{w}}_R(x - d, y)\big)
}{
\|\mathbf{w}_L(x, y) - \overline{\mathbf{w}}_L(x, y)\|_2 
\|\mathbf{w}_R(x - d, y) - \overline{\mathbf{w}}_R(x - d, y)\|_2
}

$$

对于局部匹配，由于深度 $z>0$ ，即视差存在，对于左图中的一个点，在右图中的对应点会偏左一些，只需要检测这个点的左边部分即可：

- 设置需要检查的视差范围 $[0,D]$；

- 对于每个像素，遍历所有视差，找到相似度最高的对应点；

- 利用左图和右图分别可以得到两个视差图（左图像素在右图找对应点，和反过来），将二者进行比较，即如果左图中点 A 匹配到右图中的点 B ，那么从右图点 B 匹配回左图时，也应该得到点 A ，如果不一致就去掉这个 outlier。

但是在这种算法下，得到的结果在物体边缘会有很多黑色区域（**半遮挡区域Half occlusions**），原因是在双目视觉下，总有一些部分是左侧可见但右侧不可见的，反之亦然。更大的窗口可以让这些区域更加平滑，但也会丢失细节。

> 此外，上述匹配算法存在一个很严重的缺陷，对于一些重复出现的纹理，如果只关注局部，匹配时可能有多个位置相对应；更极端一点，如果表面没有纹理，那更匹配不上了，所以还需要考虑到全局特征，全局特征也能帮助恢复上面的黑色区域
> 
> （From [思源笔记](https://lihua5487.github.io/Notes/CV/7%20Two-View%20Stereo)）

## Global Stereo Matching全局匹配

局部匹配独立地寻找每个窗口的最佳匹配，但是要想在全局拥有较好的匹配效果，我们需要引入 **Non-Local Constraints非局部约束**。

- Uniqueness：一个点在另一个图至多只能有一个匹配点；

- Ordering：对应点应该以相同的顺序出现；

- Smoothness：相邻的点应具有相似的视差值，即视差应平滑。

### Desparity Space Image视差空间图像

在这种约束下，我们构造一个图：**Desparity Space Image（DSI，视差空间图像）**

我们可以构造一种矩阵（或图

其横轴表示左图的扫描线，纵轴代表右图的扫描线，每个像素值表示其**匹配代价**。**有效区域**集中在整个矩阵的对角线附近，例如考虑视差值范围 $d\in[0,64]$，左下角是 $d<0$ 的无效区域，右上角则是 $d>64$ 的无效区域。

整个矩阵的大小是 $64N\times 64N$ 的，我们将有效值的对角带重新排列为一个矩形数组（在此情况下为64xN的大小），这就是所谓的视差空间图（DSI）。DSI中的每一条路径都是一个立体匹配，我们希望找到成本最低的路径。

视差空间图中的对应关系还需要考虑局部匹配中出现的遮挡问题：

- Matched（匹配点）：如果左右像素可以匹配，就增加一个匹配代价（Match cost）

- Occluded（遮挡点）：对于这些像素，我们需要添加一个“未匹配代价”，通常是一个较大的固定代价，用于惩罚遮挡。

![](https://lh314-pku.github.io/notes/CV/images/occluded.png)

对于每条路径，总代价就是每一步的匹配代价之和。我们需要在视差空间图像中找到**最低代价的路径**，以获取最佳匹配。

### DP（Dynamic Programming动态规划）

想要在多维有代价地图查询代价最小路径，最有效的方法是 **动·态·规·划**（来自大一上计算概论A的怨念）。代价的计算方式为：

$$
C(i,j)=\min_k(C(i,j),e_{kj}+C(k, j-1))
$$

即：到达点 $(i,j)$ 的最小代价为匹配点代价加上前一个的最小代价。

初始化：

```python
import numpy as np
# 初始化
n, m = 5, 7
C = np.full((n, m), float('inf'))
e = np.random.rand(n, m)   # 应该用 SSD 等算法实现
# 初始化起始情况
for i in range(n):
  C[i][0] = 0
# 更新代价
for j in range(1, m):
  for i in range(n):
    for k in range(n):
      C[i][j] = min(C[i][j], e[k][j] + C[k][j- 1])
```

动态规划算法最重要的就是状态转移方程。我们将上述二维矩阵存储为 $C$，用于存储从起点到任意点的最小代价。考虑转移方程：

$$
\begin{align}
C(i,j)=&\min([\\
&C(i-1,j-i)+e(i,j),\\
&C(i,j-1)+OcclusionConst,\\
&C(i-1,j)+OcclusionConst])
\end{align}
$$

伪代码：

```cpp
for(i=1; i<= N; i++) {
    for(j=1; j<= M; j++) {
        min1 = C(i-1, j-1) + e(z1,i, z2,j);
        min2 = C(i-1, j) + Occlusion;
        min3 = C(i, j-1) + Occlusion;
        C(i,j) = cmin = min(min1, min2, min3);

        if(min1 == cmin) B(i,j) = 1;
        if(min2 == cmin) B(i,j) = 2;
        if(min3 == cmin) B(i,j) = 3;
    }
}
```

最后对于被遮挡的像素，我们继续填充。用扫描线中其前面的最近有效像素的值填充左侧被遮挡的像素；同样，对于右侧被遮挡的像素，查找右侧的有效像素。

> 同样的，动态规划还可以用来拉伸图片、将不规则的图片裁剪为矩形等。只需要找到合适的状态转移方程。

## Stereo Matching via Global Optimization

全局优化下立体匹配，不仅仅局限于逐条扫描线的优化，而是将整个视差图像作为整体进行优化。全局优化能更好地解决场景中的一致性和光滑性问题。

我们想要找到一个视差图 $d(x,y)$，不仅使得像素匹配的代价最小，同时保证视差图的平滑性。我们定义视差图的**能量函数**：

$$
E(d)=E_d(d)+\lambda E_s(d)
$$

- $E_d(d)$：数据项（Data term），表示每个像素的匹配质量；

- $E_s(d)$：平滑项（Smoothness term），表示视差图的平滑性。

二者的目标是平衡匹配一致和视差平滑。

动态规划适用于逐条扫描线的优化（1D），只能处理基本的视差平滑性，效果有限。

### 数据项函数

数据项衡量的是像素之间的匹配代价。常见的计算方法包括：

$$
E_d(d)=\sum_{(x,y)\in I}C(x,y,d(x,y))
$$

其中C表示左图像素$(i,j)$在视差$d$下的匹配代价，可以用SSD或ZNCC计算。

### 平滑项函数

光滑性项通过约束邻域像素的视差变化来保证视差图的连续性和平滑性。定义为：

$$
E_s(d)=\sum_{(p,q)\in\mathcal{E}}V(d_p,d_q)
$$

- $\mathcal{E}$：邻域像素对的集合，表示哪些像素是相邻的。可以考虑4邻域/8邻域（4/8-connected neighborhood）

- $V$：表示两个相邻像素的视差代价，例如1-范数/2-范数。

至于参数 $\lambda$，表示数据与平滑项的权衡。

可以利用 graph cuts 等算法来最小化这种形式的能量函数，此外，还可以利用之前讲过的 Joint Bilateral Upsampling 方法，先降采样再升采样以加速计算

## Deep Learning

当然还可以利用深度学习的方法来预测深度图，训练时，提供一组双目图，将左图输入到网络去预测一个反向深度图（视差图），结合这个预测的反向深度图与输入对应的右图，能还原出一个左图，把它和输入的左图进行比较，作为误差。
